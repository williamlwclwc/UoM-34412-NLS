{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondace6b366db1474cec8ef40ed14bbf24f4",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports and definitions\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "import string\n",
    "from copy import deepcopy\n",
    "corpusB_path = \"./corpusB/\"\n",
    "corpusC_path = \"./corpusC/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions that can be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['abstraction', 'actually', 'add', 'address', 'answer', 'argument', 'arguments', 'back', 'call', 'car', 'case', 'cdr', 'computer', 'course', 'dictionary', 'different', 'evaluator', 'function', 'general', 'got', 'idea', 'kind', 'lambda', 'machine', 'mean', 'object', 'operator', 'order', 'pair', 'part', 'particular', 'pattern', 'place', 'problem', 'process', 'product', 'program', 'reason', 'register', 'result', 'set', 'simple', 'structure', 'system', 'they', 'together', 'using', 'variable', 'why', 'zero']\n['abstraction', 'noitcartsba', 'actually', 'yllautca', 'add', 'dda', 'address', 'sserdda', 'answer', 'rewsna', 'argument', 'tnemugra', 'arguments', 'stnemugra', 'back', 'kcab', 'call', 'llac', 'car', 'rac', 'case', 'esac', 'cdr', 'rdc', 'computer', 'retupmoc', 'course', 'esruoc', 'dictionary', 'yranoitcid', 'different', 'tnereffid', 'evaluator', 'rotaulave', 'function', 'noitcnuf', 'general', 'lareneg', 'got', 'tog', 'idea', 'aedi', 'kind', 'dnik', 'lambda', 'adbmal', 'machine', 'enihcam', 'mean', 'naem', 'object', 'tcejbo', 'operator', 'rotarepo', 'order', 'redro', 'pair', 'riap', 'part', 'trap', 'particular', 'ralucitrap', 'pattern', 'nrettap', 'place', 'ecalp', 'problem', 'melborp', 'process', 'ssecorp', 'product', 'tcudorp', 'program', 'margorp', 'reason', 'nosaer', 'register', 'retsiger', 'result', 'tluser', 'set', 'tes', 'simple', 'elpmis', 'structure', 'erutcurts', 'system', 'metsys', 'they', 'yeht', 'together', 'rehtegot', 'using', 'gnisu', 'variable', 'elbairav', 'why', 'yhw', 'zero', 'orez']\n"
    }
   ],
   "source": [
    "# read target words list\n",
    "target_words = []\n",
    "with open(\"./target-words.txt\", \"r\") as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "for target_word in words:\n",
    "    target_words.append(target_word.strip(\"\\n\"))\n",
    "\n",
    "print(target_words)\n",
    "\n",
    "# build reversed target words\n",
    "reversed_target_words = []\n",
    "for target_word in target_words:\n",
    "    reversed_target_words.append(target_word)\n",
    "    reversed_target_words.append(target_word[::-1])\n",
    "\n",
    "print(reversed_target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read corpus words\n",
    "def load_corpus(path, stem=False):\n",
    "    stemmer = PorterStemmer()\n",
    "    corpus = []\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        with open(path + file, \"r\") as f:\n",
    "            # read whole file in a string\n",
    "            content = f.read()\n",
    "            # remove symbols and numbers\n",
    "            content = content.replace(\"\\n\", \" \")\n",
    "            content = content.lower()\n",
    "            content = content.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            for i in range(10):\n",
    "                content = content.replace(str(i), \"\")\n",
    "            content = content.split(\" \")\n",
    "            # remove stop words\n",
    "            stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "            filtered_content = [w for w in content if not w in stop_words]\n",
    "\n",
    "            # choose if stemming is needed\n",
    "            if stem:\n",
    "                stemmed_content = []\n",
    "                for plural in filtered_content:\n",
    "                    if plural not in target_words:\n",
    "                        stemmed_content.append(stemmer.stem(plural))\n",
    "                    else:\n",
    "                        stemmed_content.append(plural)\n",
    "                filtered_content = stemmed_content \n",
    "            corpus.append(filtered_content)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# get the vocabulary words of a corpus\n",
    "def get_vocab_words(corpus):\n",
    "    vocab_words = []\n",
    "    for tokens in corpus:\n",
    "        vocab_words.extend(tokens)\n",
    "    vocab_words = list(set(vocab_words))\n",
    "\n",
    "    return vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a target words vocabulary words matrix of a corpus\n",
    "def build_target_vocab_matrix(corpus, target_words, vocab_words):\n",
    "    vocab_dict = {}\n",
    "    index_list = []\n",
    "    for i in range(len(vocab_words)):\n",
    "        index_list.append(i)\n",
    "    vocab_dict = dict(zip(vocab_words, index_list))\n",
    "\n",
    "    target_vocab_matrix = []\n",
    "    for target_word in target_words:\n",
    "        matrix_row = []\n",
    "        for vocab_word in vocab_words:\n",
    "            matrix_row.append(0)\n",
    "        target_vocab_matrix.append(matrix_row)\n",
    "\n",
    "    for i in range(len(target_words)):\n",
    "        for context_tokens in corpus:\n",
    "            num_oc = context_tokens.count(target_words[i])\n",
    "            if num_oc > 0:\n",
    "                for word in context_tokens:\n",
    "                    index = vocab_dict[word]\n",
    "                    if word != target_words[i]:\n",
    "                        target_vocab_matrix[i][index] += num_oc\n",
    "                    else:\n",
    "                        target_vocab_matrix[i][index] += 1\n",
    "    \n",
    "    return np.array(target_vocab_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kmeans cluster and return the label of each target word\n",
    "def cluster_words(n_clusters, target_vocab_matrix):\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(target_vocab_matrix)\n",
    "\n",
    "    return kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each target word, randomly sample half of its occurances and reverse them\n",
    "# return the reversed corpus\n",
    "def build_randomly_reverse_corpus(corpus, target_words):\n",
    "    reversed_corpus = deepcopy(corpus)\n",
    "    for target_word in target_words:\n",
    "        occur_list = []\n",
    "        for k in range(len(reversed_corpus)):\n",
    "            for i in range(len(reversed_corpus[k])):\n",
    "                if reversed_corpus[k][i] == target_word:\n",
    "                    occur_list.append([k, i])\n",
    "        random_num = len(occur_list) // 2\n",
    "        random_index_list = random.sample(occur_list, random_num)\n",
    "        for index in random_index_list:\n",
    "            reversed_corpus[index[0]][index[1]] = target_word[::-1]\n",
    "\n",
    "    return reversed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus based on a smaller context window size\n",
    "def build_corpus(context_size, corpus, target_words):\n",
    "    new_corpus = []\n",
    "    for words_array in corpus:\n",
    "        for i in range(len(words_array)):\n",
    "            if words_array[i] in target_words:\n",
    "                if i-context_size >= 0:\n",
    "                    slice_start = i-context_size\n",
    "                else:\n",
    "                    slice_start = 0\n",
    "                if i+context_size+1 <= len(words_array):\n",
    "                    slice_end = i+context_size+1\n",
    "                else:\n",
    "                    slice_end = len(words_array)\n",
    "                context = words_array[slice_start:slice_end]\n",
    "                new_corpus.append(context)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Calculate Target Words Vocabulary Words Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "matrix shape: (50, 5711)\n[[23281    48     7 ...     0     0     0]\n [ 1320    22     2 ...     0     0     0]\n [21048    27     0 ...     0     0     0]\n ...\n [45364   120     5 ...     0     0     0]\n [    0     0     0 ...     0     0     0]\n [ 9386     7     2 ...     0     0     0]]\n[ 4  1 34 23 44 21  0 45 25 35 39 14 33 47 48 29 10 22 26 49 18 37 17  9\n 46 19 31  3 32 15 41  8 40 36  7 38 27 43  2 28 11 16 20  5  6 42 12 13\n 24 30]\n"
    }
   ],
   "source": [
    "# print matrix for corpusB as an example\n",
    "corpusB = load_corpus(corpusB_path)\n",
    "vocab_words = get_vocab_words(corpusB)\n",
    "target_vocab_matrix = build_target_vocab_matrix(corpusB, target_words, vocab_words)\n",
    "print(\"matrix shape: \" + str(target_vocab_matrix.shape))\n",
    "print(target_vocab_matrix)\n",
    "result = cluster_words(50, target_vocab_matrix)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Apply Cluster on CorpusB with whole document\n",
    "This could be the standard for (c) analysis to compare with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[24 24  4 18 38 38 40 40  4 35 22 44 47 48 18 18 31 31 33 11 49 49 33 11\n 26 26 18 18 18 18 41 41 30 12 21 21  2  2 18 18 35 35  4  4 15 15 17  3\n 18 18 36 46  1  1  8 19 42 42 43 43 35 35 25  7  4  4 13 13  6 45 29 29\n 16 39 18 18 23  9 28 28 27  0 34 34 37 37  5 32 13 13 35 35 14 14 20 10\n 18 18 29 29]\nCorrect Pairs: 22\nAccuracy: 0.44\n[ 8  8 16 16 30 30 44 44  2  2 10 10 28 40 16 16 29 29 24 24  7  7  0 24\n 39 49 35 35 35 35 25 25 22 12 18 18 17 37 35 35 47 47  2  2 27 27 23 19\n 35 35 36 41 14 14 45  1  0  0 31 31 47 47 21  3  2  2 46 46 42  9 32 32\n 43 28 16 35 20  5 15 15 11 26 17 17 34 34 48 13  7  7 47 47  4 38 33  6\n 35 35 32 32]\nCorrect Pairs: 22\nAccuracy: 0.44\n[20 35  6  6 35 37 17 17 22 22 39 49 23 23  6  6  0  0 15 15 38 38 15 15\n 44 46 30 30 30 30 32 32 29  9 28 36 25 25 30 30 22 22 22 22 48  0 26  4\n 30 30 12 27 45 45  1 19  5  5 43 43 22 22 31  7 22  6  2  2 33 13 34 34\n 47 14 30  6 24 10 27 27 18  8 40 40 42 42 21  3 38 38 22 22 11 11 41 16\n 30 30 34 34]\nCorrect Pairs: 19\nAccuracy: 0.38\n[17 17 41 41 28 28 42 42  8  8 38 38 21 39 41 41  2  2 33 33 26 26 46 33\n 35 44  0  0  0  0  7  7 13 18 23 23 36 36  0  0  8  8 41  8 40 40 20 16\n  0  0 49 27 14 14 31  5 15 15 14 26  8  8 29 10  8 41 43 43  1  1 32 32\n 21 12  0  0 24  3 19 48 25 11  4  4 30 47 34  6 26 26  8 26 22 37 45  9\n  0  0 32 32]\nCorrect Pairs: 20\nAccuracy: 0.4\n[ 5  5  4  4 38 35 41 17 24 24 37 44 30 30  4  4 39 23 47 16  0  0 47 45\n 29 29 26 26 26 26 33 33  1 21 27 27  9  9 26 26 48 48 24 24 23 23 13 28\n 26 26 25 25 17 17 22  3 18 18 29 43 24 48 31  7 24  4  0 43 36 11 15 15\n 30  2  4 26 19  8 34 34 20 12 40 49 46 42 10 10  0  0 48 48 14 14 32  6\n 26 26 15 15]\nCorrect Pairs: 19\nAccuracy: 0.38\nOverall Accuracy: 0.40800000000000003\n"
    }
   ],
   "source": [
    "overall_accuracy = 0\n",
    "corpusB = load_corpus(corpusB_path)\n",
    "for k in range(5):\n",
    "    reversed_corpusB = build_randomly_reverse_corpus(corpusB, target_words)\n",
    "    vocab_words = get_vocab_words(reversed_corpusB)\n",
    "    target_vocab_matrix = build_target_vocab_matrix(reversed_corpusB, reversed_target_words, vocab_words)\n",
    "\n",
    "    result = cluster_words(50, target_vocab_matrix)\n",
    "    print(result)\n",
    "\n",
    "    cnt_correct = 0\n",
    "    for i in range(50):\n",
    "        index_list = list(np.where(result==i))[0]\n",
    "        index_list.sort()\n",
    "        for j in range(len(index_list)-1):\n",
    "            if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                cnt_correct += 1\n",
    "                break    \n",
    "\n",
    "    print(\"Correct Pairs: \" + str(cnt_correct))\n",
    "    print(\"Accuracy: \" + str(cnt_correct / 50))\n",
    "\n",
    "    overall_accuracy += cnt_correct / 50\n",
    "\n",
    "print(\"Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Change type of features to stemmed vocabulary words\n",
    "Using stemmed corpusB to do clustering, it seems that there are not much difference compared with (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Correct Pairs: 21\nAccuracy: 0.42\nCorrect Pairs: 22\nAccuracy: 0.44\nCorrect Pairs: 22\nAccuracy: 0.44\nCorrect Pairs: 21\nAccuracy: 0.42\nCorrect Pairs: 18\nAccuracy: 0.36\nOverall Accuracy: 0.41600000000000004\n"
    }
   ],
   "source": [
    "overall_accuracy = 0\n",
    "corpusB = load_corpus(corpusB_path, True)\n",
    "for k in range(5):\n",
    "    reversed_corpusB = build_randomly_reverse_corpus(corpusB, target_words)\n",
    "    vocab_words = get_vocab_words(reversed_corpusB)\n",
    "    target_vocab_matrix = build_target_vocab_matrix(reversed_corpusB, reversed_target_words, vocab_words)\n",
    "\n",
    "    result = cluster_words(50, target_vocab_matrix)\n",
    "    \n",
    "    cnt_correct = 0\n",
    "    for i in range(50):\n",
    "        index_list = list(np.where(result==i))[0]\n",
    "        index_list.sort()\n",
    "        for j in range(len(index_list)-1):\n",
    "            if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                cnt_correct += 1\n",
    "                break    \n",
    "\n",
    "    print(\"Correct Pairs: \" + str(cnt_correct))\n",
    "    print(\"Accuracy: \" + str(cnt_correct / 50))\n",
    "\n",
    "    overall_accuracy += cnt_correct / 50\n",
    "\n",
    "print(\"Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Change size of context to different window size contexts\n",
    "The accuracy increases when the size of context increases, the maximum accuracy is around 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Context size 5 Overall Accuracy: 0.10800000000000001\nContext size 10 Overall Accuracy: 0.308\nContext size 15 Overall Accuracy: 0.404\nContext size 20 Overall Accuracy: 0.512\nContext size 25 Overall Accuracy: 0.624\nContext size 30 Overall Accuracy: 0.6799999999999999\nContext size 35 Overall Accuracy: 0.712\nContext size 40 Overall Accuracy: 0.728\n"
    }
   ],
   "source": [
    "for size in range(5, 41, 5):\n",
    "    # change context size\n",
    "    corpusB = load_corpus(corpusB_path)\n",
    "    corpusB = build_corpus(size, corpusB, target_words)\n",
    "    overall_accuracy = 0\n",
    "    for k in range(5):\n",
    "        reversed_corpusB = build_randomly_reverse_corpus(corpusB, target_words)\n",
    "        vocab_words = get_vocab_words(reversed_corpusB)\n",
    "        target_vocab_matrix = build_target_vocab_matrix(reversed_corpusB, reversed_target_words, vocab_words)\n",
    "\n",
    "        result = cluster_words(50, target_vocab_matrix)\n",
    "        \n",
    "        cnt_correct = 0\n",
    "        for i in range(50):\n",
    "            index_list = list(np.where(result==i))[0]\n",
    "            index_list.sort()\n",
    "            for j in range(len(index_list)-1):\n",
    "                if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                    cnt_correct += 1\n",
    "                    break    \n",
    "\n",
    "        overall_accuracy += cnt_correct / 50\n",
    "\n",
    "    print(\"Context size \" + str(size) + \" Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) training data on the quality of generated clusters\n",
    "The quality of corpusC seems better than corpusB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train cluster on copusC on whole documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Correct Pairs: 43\nAccuracy: 0.86\nCorrect Pairs: 37\nAccuracy: 0.74\nCorrect Pairs: 40\nAccuracy: 0.8\nCorrect Pairs: 38\nAccuracy: 0.76\nCorrect Pairs: 35\nAccuracy: 0.7\nOverall Accuracy: 0.772\n"
    }
   ],
   "source": [
    "overall_accuracy = 0\n",
    "corpusC = load_corpus(corpusC_path)\n",
    "for k in range(5):\n",
    "    reversed_corpusC = build_randomly_reverse_corpus(corpusC, target_words)\n",
    "    vocab_words = get_vocab_words(reversed_corpusC)\n",
    "    target_vocab_matrix = build_target_vocab_matrix(reversed_corpusC, reversed_target_words, vocab_words)\n",
    "\n",
    "    result = cluster_words(50, target_vocab_matrix)\n",
    "    \n",
    "    cnt_correct = 0\n",
    "    for i in range(50):\n",
    "        index_list = list(np.where(result==i))[0]\n",
    "        index_list.sort()\n",
    "        for j in range(len(index_list)-1):\n",
    "            if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                cnt_correct += 1\n",
    "                break    \n",
    "\n",
    "    print(\"Correct Pairs: \" + str(cnt_correct))\n",
    "    print(\"Accuracy: \" + str(cnt_correct / 50))\n",
    "\n",
    "    overall_accuracy += cnt_correct / 50\n",
    "\n",
    "print(\"Overall Accuracy: \" + str(overall_accuracy / 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on corpusC and using smaller context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Context size 5 Overall Accuracy: 0.20800000000000002\nContext size 10 Overall Accuracy: 0.808\nContext size 15 Overall Accuracy: 0.9279999999999999\nContext size 20 Overall Accuracy: 0.944\nContext size 25 Overall Accuracy: 0.96\nContext size 30 Overall Accuracy: 0.96\nContext size 35 Overall Accuracy: 0.96\nContext size 40 Overall Accuracy: 0.96\n"
    }
   ],
   "source": [
    "for size in range(5, 41, 5):\n",
    "    overall_accuracy = 0\n",
    "    corpusC = load_corpus(corpusC_path)\n",
    "    corpusC = build_corpus(size, corpusC, target_words)\n",
    "    for k in range(5):\n",
    "        reversed_corpusC = build_randomly_reverse_corpus(corpusC, target_words)\n",
    "        vocab_words = get_vocab_words(reversed_corpusC)\n",
    "        target_vocab_matrix = build_target_vocab_matrix(reversed_corpusC, reversed_target_words, vocab_words)\n",
    "\n",
    "        result = cluster_words(50, target_vocab_matrix)\n",
    "        \n",
    "        cnt_correct = 0\n",
    "        for i in range(50):\n",
    "            index_list = list(np.where(result==i))[0]\n",
    "            index_list.sort()\n",
    "            for j in range(len(index_list)-1):\n",
    "                if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                    cnt_correct += 1\n",
    "                    break    \n",
    "\n",
    "        overall_accuracy += cnt_correct / 50\n",
    "\n",
    "    print(\"Context size \" + str(size) + \" Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train cluster on combination of corpusB and corpusC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Correct Pairs: 33\nAccuracy: 0.66\nCorrect Pairs: 35\nAccuracy: 0.7\nCorrect Pairs: 35\nAccuracy: 0.7\nCorrect Pairs: 36\nAccuracy: 0.72\nCorrect Pairs: 35\nAccuracy: 0.7\nOverall Accuracy: 0.696\n"
    }
   ],
   "source": [
    "corpusC = load_corpus(corpusC_path)\n",
    "corpusBC = load_corpus(corpusB_path)\n",
    "corpusBC.extend(corpusC)\n",
    "\n",
    "overall_accuracy = 0\n",
    "for k in range(5):\n",
    "    reversed_corpusBC = build_randomly_reverse_corpus(corpusBC, target_words)\n",
    "    vocab_words = get_vocab_words(reversed_corpusBC)\n",
    "    target_vocab_matrix = build_target_vocab_matrix(reversed_corpusBC, reversed_target_words, vocab_words)\n",
    "    \n",
    "    result = cluster_words(50, target_vocab_matrix)\n",
    "    \n",
    "    cnt_correct = 0\n",
    "    for i in range(50):\n",
    "        index_list = list(np.where(result==i))[0]\n",
    "        index_list.sort()\n",
    "        for j in range(len(index_list)-1):\n",
    "            if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                cnt_correct += 1\n",
    "                break    \n",
    "\n",
    "    print(\"Correct Pairs: \" + str(cnt_correct))\n",
    "    print(\"Accuracy: \" + str(cnt_correct / 50))\n",
    "\n",
    "    overall_accuracy += cnt_correct / 50\n",
    "\n",
    "print(\"Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train cluster on combination of corpusB and corpusC using smaller context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Context size 5 Overall Accuracy: 0.156\nContext size 10 Overall Accuracy: 0.5\nContext size 15 Overall Accuracy: 0.664\nContext size 20 Overall Accuracy: 0.784\nContext size 25 Overall Accuracy: 0.8880000000000001\nContext size 30 Overall Accuracy: 0.9200000000000002\nContext size 35 Overall Accuracy: 0.8960000000000001\nContext size 40 Overall Accuracy: 0.9200000000000002\n"
    }
   ],
   "source": [
    "for size in range(5, 41, 5):\n",
    "    corpusB = load_corpus(corpusB_path)\n",
    "    corpusBC = load_corpus(corpusC_path)\n",
    "    corpusBC.extend(corpusB)\n",
    "    corpusBC = build_corpus(size, corpusBC, target_words)\n",
    "\n",
    "    overall_accuracy = 0\n",
    "    for k in range(5):\n",
    "        reversed_corpusBC = build_randomly_reverse_corpus(corpusBC, target_words)\n",
    "        vocab_words = get_vocab_words(reversed_corpusBC)\n",
    "        target_vocab_matrix = build_target_vocab_matrix(reversed_corpusBC, reversed_target_words, vocab_words)\n",
    "        \n",
    "        result = cluster_words(50, target_vocab_matrix)\n",
    "        \n",
    "        cnt_correct = 0\n",
    "        for i in range(50):\n",
    "            index_list = list(np.where(result==i))[0]\n",
    "            index_list.sort()\n",
    "            for j in range(len(index_list)-1):\n",
    "                if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                    cnt_correct += 1\n",
    "                    break    \n",
    "\n",
    "        overall_accuracy += cnt_correct / 50\n",
    "\n",
    "    print(\"Context size \" + str(size) + \" Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}