{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Words Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports and others\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "corpusB_path = \"./corpusB/\"\n",
    "corpusC_path = \"./corpusC/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstraction', 'actually', 'add', 'address', 'answer', 'argument', 'arguments', 'back', 'call', 'car', 'case', 'cdr', 'computer', 'course', 'dictionary', 'different', 'evaluator', 'function', 'general', 'got', 'idea', 'kind', 'lambda', 'machine', 'mean', 'object', 'operator', 'order', 'pair', 'part', 'particular', 'pattern', 'place', 'problem', 'process', 'product', 'program', 'reason', 'register', 'result', 'set', 'simple', 'structure', 'system', 'they', 'together', 'using', 'variable', 'why', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# read target words list\n",
    "target_words = []\n",
    "with open(\"./target-words.txt\", \"r\") as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "for target_word in words:\n",
    "    target_words.append(target_word.strip(\"\\n\"))\n",
    "\n",
    "print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read corpus words\n",
    "corpus = []\n",
    "files = os.listdir(corpusB_path)\n",
    "for file in files:\n",
    "    with open(corpusB_path + file, \"r\") as f:\n",
    "        # read whole file in a string\n",
    "        content = f.read()\n",
    "        content = content.replace(\"\\n\", \"\")\n",
    "        corpus.append(content)\n",
    "\n",
    "# print(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_vocab_matrix(corpus, target_words):\n",
    "    # build doc-term matrix\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    vocablulary_words = vectorizer.get_feature_names()\n",
    "    word_id_dict = vectorizer.vocabulary_\n",
    "    doc_term_matrix = np.array(X.toarray())\n",
    "\n",
    "    # build target_vocab_matrix\n",
    "    term_term_matrix = []\n",
    "    for word in target_words:\n",
    "        index = word_id_dict.get(word)\n",
    "        if index is not None:\n",
    "            target_list = doc_term_matrix[:, index]\n",
    "            term_term_row = []\n",
    "            for vocab_word in vocablulary_words:\n",
    "                vocab_index = word_id_dict.get(vocab_word)\n",
    "                vocab_list = doc_term_matrix[:, vocab_index]\n",
    "                term_term_row.append(np.dot(target_list, vocab_list))\n",
    "            term_term_matrix.append(term_term_row)\n",
    "\n",
    "    term_term_matrix = np.array(term_term_matrix)\n",
    "    # print(term_term_matrix.shape)\n",
    "    return term_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_target_words(corpus, target_words, num_clusters):\n",
    "    target_vocab = target_vocab_matrix(corpus, target_words)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=None).fit(target_vocab)\n",
    "    # print(kmeans.labels_)\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstraction', 'noitcartsba', 'actually', 'yllautca', 'add', 'dda', 'address', 'sserdda', 'answer', 'rewsna', 'argument', 'tnemugra', 'arguments', 'stnemugra', 'back', 'kcab', 'call', 'llac', 'car', 'rac', 'case', 'esac', 'cdr', 'rdc', 'computer', 'retupmoc', 'course', 'esruoc', 'dictionary', 'yranoitcid', 'different', 'tnereffid', 'evaluator', 'rotaulave', 'function', 'noitcnuf', 'general', 'lareneg', 'got', 'tog', 'idea', 'aedi', 'kind', 'dnik', 'lambda', 'adbmal', 'machine', 'enihcam', 'mean', 'naem', 'object', 'tcejbo', 'operator', 'rotarepo', 'order', 'redro', 'pair', 'riap', 'part', 'trap', 'particular', 'ralucitrap', 'pattern', 'nrettap', 'place', 'ecalp', 'problem', 'melborp', 'process', 'ssecorp', 'product', 'tcudorp', 'program', 'margorp', 'reason', 'nosaer', 'register', 'retsiger', 'result', 'tluser', 'set', 'tes', 'simple', 'elpmis', 'structure', 'erutcurts', 'system', 'metsys', 'they', 'yeht', 'together', 'rehtegot', 'using', 'gnisu', 'variable', 'elbairav', 'why', 'yhw', 'zero', 'orez']\n"
     ]
    }
   ],
   "source": [
    "# build reversed target words\n",
    "reversed_target_words = []\n",
    "for target_word in target_words:\n",
    "    reversed_target_words.append(target_word)\n",
    "    reversed_target_words.append(target_word[::-1])\n",
    "\n",
    "print(reversed_target_words)\n",
    "# print(len(reversed_target_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build randomly reversed corpusB\n",
    "reversed_corpus = []\n",
    "for doc in corpus:\n",
    "    new_doc = []\n",
    "    tokens = doc.split(\" \")\n",
    "    for token in tokens:\n",
    "        if token in target_words:\n",
    "            if random.random() > 0.5:\n",
    "                token = token[::-1]\n",
    "        new_doc.append(token)\n",
    "    new_doc = \" \".join(new_doc)\n",
    "    reversed_corpus.append(new_doc)\n",
    "\n",
    "# print(len(reversed_corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Pairs: 7\n",
      "Accuracy: 0.14\n",
      "Correct Pairs: 6\n",
      "Accuracy: 0.12\n",
      "Correct Pairs: 6\n",
      "Accuracy: 0.12\n",
      "Correct Pairs: 6\n",
      "Accuracy: 0.12\n",
      "Correct Pairs: 5\n",
      "Accuracy: 0.1\n",
      "Overall Accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = 0\n",
    "for i in range(5):\n",
    "    result = cluster_target_words(reversed_corpus, reversed_target_words, 50)\n",
    "    # print(result)\n",
    "    cnt_correct = 0\n",
    "    for i in range(50):\n",
    "        index_list = list(np.where(result==i))[0]\n",
    "        for j in range(len(index_list)-1):\n",
    "            if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                cnt_correct += 1\n",
    "                break    \n",
    "\n",
    "    print(\"Correct Pairs: \" + str(cnt_correct))\n",
    "    print(\"Accuracy: \" + str(cnt_correct / 50))\n",
    "\n",
    "    overall_accuracy += cnt_correct / 50\n",
    "\n",
    "print(\"Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus\n",
    "def build_corpus(context_size, corpus, target_words):\n",
    "    new_corpus = []\n",
    "    for target_word in target_words:\n",
    "        for doc in corpus:\n",
    "            words_array = doc.split(\" \")\n",
    "            for i in range(len(words_array)):\n",
    "                if words_array[i] in target_words:\n",
    "                    if i-context_size >= 0:\n",
    "                        slice_start = i-context_size\n",
    "                    else:\n",
    "                        slice_start = 0\n",
    "                    if i+context_size <= len(words_array):\n",
    "                        slice_end = i+context_size\n",
    "                    else:\n",
    "                        slice_end = len(words_array)\n",
    "                    context = words_array[slice_start:slice_end]\n",
    "                    context = \" \".join(context)\n",
    "                    new_corpus.append(context)\n",
    "\n",
    "    return new_corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusB_win5 = build_corpus(5, reversed_corpus, reversed_target_words)\n",
    "overall_accuracy = 0\n",
    "for i in range(5):\n",
    "    result = cluster_target_words(corpusB_win5, reversed_target_words, 50)\n",
    "    # print(result)\n",
    "    cnt_correct = 0\n",
    "    for i in range(50):\n",
    "        index_list = list(np.where(result==i))[0]\n",
    "        for j in range(len(index_list)-1):\n",
    "            if index_list[j] + 1 == index_list[j+1] and index_list[j] % 2 == 0:\n",
    "                cnt_correct += 1\n",
    "                break    \n",
    "\n",
    "    print(\"Correct Pairs: \" + str(cnt_correct))\n",
    "    print(\"Accuracy: \" + str(cnt_correct / 50))\n",
    "\n",
    "    overall_accuracy += cnt_correct / 50\n",
    "\n",
    "print(\"Overall Accuracy: \" + str(overall_accuracy / 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondace6b366db1474cec8ef40ed14bbf24f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
